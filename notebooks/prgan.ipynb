{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n",
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0\n",
    "%env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "from torch.nn import utils\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.transforms import Compose, ToTensor, Resize, RandomHorizontalFlip\n",
    "\n",
    "from data_utils import CelebaDataset, BatchCollate\n",
    "from models import Generator, ConcatGenerator, ProjectionDiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 256)\n",
    "y = torch.randint(0, 4, (4,))\n",
    "# y = nn.functional.one_hot(torch.randint(0, 4, (4,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7402, 0.5766, 1.4744,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 2.9183, 0.0000,  ..., 0.4084, 0.0000, 2.3889],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 2.8478, 0.0000, 0.0000],\n",
       "        [1.6395, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(4, 256)\n",
    "y = torch.randint(0, 2, (4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2372],\n",
       "        [ 0.1720],\n",
       "        [-0.4609],\n",
       "        [ 0.0255]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ProjectionDiscriminator(num_classes=4)(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms = Compose([\n",
    "#     Resize((64, 64)),\n",
    "#     RandomHorizontalFlip(),\n",
    "#     ToTensor()\n",
    "# ])\n",
    "\n",
    "# features = ['Black_Hair']\n",
    "# dataset = CelebaDataset(usecols=features,\n",
    "#                         attr_file='CelebA/list_attr_celeba.txt', \n",
    "#                         dataset_location='CelebA/img_align_celeba/', \n",
    "#                         transforms=transforms)\n",
    "\n",
    "# image, label = dataset[np.random.choice(len(dataset))]\n",
    "# print('\\n'.join([name + f\": {val}\" for name, val in zip(features, dataset.one2multi[label])]))\n",
    "# plt.imshow(image.permute(1, 2, 0))\n",
    "# plt.axis('off');\n",
    "\n",
    "# image, label = dataset[np.random.choice(len(dataset))]\n",
    "# print('\\n'.join([name + f\": {val}\" for name, val in zip(features, dataset.one2multi[label])]))\n",
    "# plt.imshow(image.permute(1, 2, 0))\n",
    "# plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_target_labels(y_source, num_classes=2):\n",
    "    return torch.randint(0, num_classes, y_source.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisLoss(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(DisLoss, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        if loss_type == 'hinge':\n",
    "            self.critetion = self.hinge_loss\n",
    "        elif loss_type == 'dcgan':\n",
    "            self.critetion = self.dcgan_loss\n",
    "        elif loss_type == 'lsgan':\n",
    "            self.critetion = self.lsgan_loss\n",
    "        else:\n",
    "            raise ValueError('Not supported! Choose from [\\'hinge\\', \\'dcgan\\', \\'lsgan\\'].')\n",
    "        \n",
    "    def hinge_loss(self, dis_real, dis_fake):\n",
    "        return (1. - dis_real).relu().mean() + (1. + dis_fake).relu().mean()\n",
    "    \n",
    "    def dcgan_loss(self, dis_real, dis_fake):\n",
    "        return -torch.log(F.softplus(-dis_real).mean().sigmoid()) - \\\n",
    "                torch.log(1.0 - F.softplus(dis_fake).mean().sigmoid())\n",
    "    \n",
    "    def lsgan_loss(self, dis_real, dis_fake):\n",
    "        return 0.5 * ((dis_real - 1).pow(2).mean() + dis_fake.pow(2).mean())\n",
    "        \n",
    "    def forward(self, dis_real, dis_fake):\n",
    "        \"\"\"\n",
    "        PARAMS:\n",
    "            dis_real (Tensor): discriminator scores for real data\n",
    "            dis_fake (Tensor): discriminator scores for fake outputs of generator\n",
    "        \"\"\"\n",
    "        return self.critetion(dis_real, dis_fake)\n",
    "    \n",
    "    \n",
    "class GenLoss(nn.Module):\n",
    "    def __init__(self, loss_type):\n",
    "        super(GenLoss, self).__init__()\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        if loss_type == 'hinge':\n",
    "            self.critetion = self.hinge_loss\n",
    "        elif loss_type == 'dcgan':\n",
    "            self.critetion = self.dcgan_loss\n",
    "        elif loss_type == 'lsgan':\n",
    "            self.critetion = self.lsgan_loss\n",
    "        else:\n",
    "            raise ValueError('Not supported! Choose from [\\'hinge\\', \\'dcgan\\', \\'lsgan\\'].')\n",
    "        \n",
    "    def hinge_loss(self, dis_fake):\n",
    "        return -dis_fake.mean()\n",
    "    \n",
    "    def dcgan_loss(self, dis_fake):\n",
    "        return -torch.log(F.softplus(-dis_fake).mean().sigmoid())\n",
    "    \n",
    "    def lsgan_loss(self, dis_fake):\n",
    "        return 0.5 * (dis_fake - 1).pow(2).mean()\n",
    "        \n",
    "    def forward(self, dis_fake):\n",
    "        \"\"\"\n",
    "        PARAMS: \n",
    "            dis_fake (Tensor): discriminator scores for fake outputs of generator\n",
    "        \"\"\"\n",
    "        return self.critetion(dis_fake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Dataset with images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = Compose([\n",
    "    Resize((64, 64)),\n",
    "    RandomHorizontalFlip(),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Black_Hair']\n",
    "# features = ['Male']\n",
    "dataset = CelebaDataset(usecols=features,\n",
    "                        attr_file='CelebA/list_attr_celeba.txt', \n",
    "                        dataset_location='CelebA/img_align_celeba/', \n",
    "                        transforms=transforms)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=4,\n",
    "                        collate_fn=BatchCollate(), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "G = Generator(hidden_size=128, num_classes=num_classes).cuda()\n",
    "D = ProjectionDiscriminator(hidden_size=128, num_classes=num_classes).cuda()\n",
    "\n",
    "args =  {'dataset': 'MNIST',\n",
    "         'eval_each': 10,\n",
    "         'epochs': 101,\n",
    "         'log_dir': 'CelebA64_256_v2/',\n",
    "         'device': 'cuda:0',\n",
    "         'weight_decay': 1e-05,\n",
    "         'depth': 16,\n",
    "         'gamma': 0.2,\n",
    "         'lmbda': 0.5,\n",
    "         'batch_norm': False,\n",
    "         'batch_size': 64,\n",
    "         'colors': 3,\n",
    "         'latent_width': 4, # Bottleneck HW\n",
    "         'width': 128, # Means 4 downsampling blocks\n",
    "         'latent': 32, # Bottleneck channels\n",
    "         'n_classes': 10,\n",
    "         'advdepth': 16,\n",
    "         'lr': 0.0001}\n",
    "\n",
    "scales = int(round(math.log(args['width'] // args['latent_width'], 2)))\n",
    "ae = Autoencoder(scales=scales,depth=args['depth'],latent=args['latent'],colors=args['colors']).to(args['device']).eval()\n",
    "\n",
    "ae.load_state_dict(torch.load('acai_64.pt', map_location='cuda:0'))\n",
    "\n",
    "optimizer_g = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "optimizer_d = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "# optimizer_g = torch.optim.Adam(G.parameters(), betas=(0, 0.9))\n",
    "# optimizer_d = torch.optim.Adam(D.parameters(), betas=(0, 0.9))\n",
    "\n",
    "loss_type = 'hinge'\n",
    "criterion_d = DisLoss(loss_type)\n",
    "criterion_g = GenLoss(loss_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "for epoch in range(10):\n",
    "    for _ in tqdm_notebook(range(len(dataloader))):\n",
    "        # =================================================================== #\n",
    "        #                         1. Get new batch                            #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        images, y_real = next(iter(dataloader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        images, y_real, y_fake = images.cuda(), y_real.cuda(), y_fake.cuda()\n",
    "        # y_real_hot = F.one_hot(y_real, num_classes)\n",
    "        # y_fake_hot = F.one_hot(y_fake, num_classes)\n",
    "        \n",
    "        bs = images.size(0)\n",
    "\n",
    "        # get latent codes of images\n",
    "        with torch.no_grad():\n",
    "            latent_real = ae.encoder(images).view(bs, -1)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        #                        2. Train Discriminator                       #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        dis_real = D(latent_real, y_real)      \n",
    "                           \n",
    "        images, y_real = next(iter(dataloader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        images, y_real, y_fake = images.cuda(), y_real.cuda(), y_fake.cuda()\n",
    "        \n",
    "        bs = images.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent_real = ae.encoder(images).view(bs, -1)\n",
    "        \n",
    "        latent_fake = G(latent_real, y_fake)\n",
    "        dis_fake = D(latent_fake, y_fake)\n",
    "\n",
    "        d_loss = criterion_d(dis_real, dis_fake)\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         3. Train Generator                          #\n",
    "        # =================================================================== #\n",
    "        images, y_real = next(iter(dataloader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        images, y_real, y_fake = images.cuda(), y_real.cuda(), y_fake.cuda()\n",
    "        \n",
    "        bs = images.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            latent_real = ae.encoder(images).view(bs, -1)\n",
    "        \n",
    "        latent_fake = G(latent_real, y_fake)\n",
    "        dis_fake = D(latent_fake, y_fake)\n",
    "        \n",
    "        latent_cyclic =  G(G(latent_real, y_fake), y_real)\n",
    "        cyclic_loss = F.l1_loss(latent_cyclic, latent_real)\n",
    "        \n",
    "        # latent_id =  G(latent_real, y_real)\n",
    "        # identity_loss = F.l1_loss(latent_id, latent_real)\n",
    "        \n",
    "        g_loss = criterion_g(dis_fake) + cyclic_loss\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            print(\"Step: {} | D_loss: {:.3f} | G_loss: {:.3f} | Cyclic loss: {:.3f}\".format(iteration, \n",
    "                                                                        d_loss.item(), \n",
    "                                                                        g_loss.item(),\n",
    "                                                                        cyclic_loss.item()))\n",
    "            \n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            with torch.no_grad():\n",
    "                latent_real = ae.encoder(images).view(bs, -1)\n",
    "                latent_fake = G(latent_real, y_fake)\n",
    "                x_decoded = ae.decoder(latent_fake.view(bs, -1, 2, 2)).detach().cpu().permute(0, 2, 3, 1)\n",
    "            \n",
    "            images = images.detach().cpu().permute(0, 2, 3, 1)\n",
    "            target = np.array([dataset.one2multi[a.item()] for a in y_fake])\n",
    "            target[target == -1] = 0\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=2, ncols=8, figsize=(20, 5))\n",
    "            for i in range(2):\n",
    "                for j in range(8):\n",
    "                    if i == 0:\n",
    "                        name = '\\n'.join([name + f\": {val}\" for name, val in zip(dataset.usecols, target[j])])\n",
    "                        ax[i][j].set_title(name, fontsize=14)\n",
    "                        ax[i][j].imshow(images[j], aspect='auto')\n",
    "                        ax[i][j].axis('off')\n",
    "                    else:\n",
    "                        ax[i][j].imshow(x_decoded[j], aspect='auto')\n",
    "                        ax[i][j].axis('off')\n",
    "            plt.tight_layout(pad=0)\n",
    "            plt.show()\n",
    "        \n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, emb):\n",
    "        self.embedding = emb\n",
    "        one_hot_labels = np.array([a[-2:].numpy() for a in emb])\n",
    "        unique_labels = np.unique(one_hot_labels, axis=0)\n",
    "        self.num_classes = unique_labels.shape[0]\n",
    "        self.multi2one = {tuple(label): i for i, label in enumerate(unique_labels)}\n",
    "        self.one2multi = {i: label.tolist() for i, label in enumerate(unique_labels)}\n",
    "        \n",
    "        self.labels = np.apply_along_axis(lambda x: self.multi2one[tuple(x)], \n",
    "                                          axis=1,\n",
    "                                          arr=one_hot_labels)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.embedding)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        obj = self.embedding[idx]\n",
    "        label = self.labels[idx]\n",
    "        while label == self.num_classes - 1:\n",
    "            idx = np.random.choice(self.__len__())\n",
    "            obj = self.embedding[idx]\n",
    "            label = self.labels[idx]\n",
    "        return (obj[:-2], label)\n",
    "\n",
    "\n",
    "class BatchCollate(object):\n",
    "    def __call__(self, batch):\n",
    "        images = torch.stack([img for img, _ in batch])\n",
    "        labels = torch.from_numpy(np.array([label for _, label in batch]))\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Black_Hair', 'Blond_Hair']\n",
    "train_dataset = EmbeddingDataset(torch.load('Embedding_CelebA64.pth'))\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                          num_workers=0, shuffle=True,\n",
    "                          collate_fn=BatchCollate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "num_classes = 2\n",
    "G = Generator(hidden_size=128, num_classes=num_classes).to(DEVICE)\n",
    "D = ProjectionDiscriminator(hidden_size=128, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "args =  {'dataset': 'MNIST',\n",
    "         'eval_each': 10,\n",
    "         'epochs': 101,\n",
    "         'log_dir': 'CelebA64_256_v2/',\n",
    "         'device': DEVICE,\n",
    "         'weight_decay': 1e-05,\n",
    "         'depth': 16,\n",
    "         'gamma': 0.2,\n",
    "         'lmbda': 0.5,\n",
    "         'batch_norm': False,\n",
    "         'batch_size': 64,\n",
    "         'colors': 3,\n",
    "         'latent_width': 4, # Bottleneck HW\n",
    "         'width': 128, # Means 4 downsampling blocks\n",
    "         'latent': 32, # Bottleneck channels\n",
    "         'n_classes': 10,\n",
    "         'advdepth': 16,\n",
    "         'lr': 0.0001}\n",
    "\n",
    "scales = int(round(math.log(args['width'] // args['latent_width'], 2)))\n",
    "ae = Autoencoder(scales=scales,depth=args['depth'],latent=args['latent'],colors=args['colors']).to(args['device']).eval()\n",
    "\n",
    "ae.load_state_dict(torch.load('acai_64.pt', map_location=args['device']))\n",
    "\n",
    "optimizer_g = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "optimizer_d = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "\n",
    "loss_type = 'hinge'\n",
    "\n",
    "criterion_d = DisLoss(loss_type)\n",
    "criterion_g = GenLoss(loss_type)\n",
    "\n",
    "G.train();\n",
    "D.train();\n",
    "\n",
    "BCE = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "dis_fakes = []\n",
    "dis_reals = []\n",
    "for epoch in range(10):\n",
    "    for _ in tqdm_notebook(range(len(train_loader))):\n",
    "        # =================================================================== #\n",
    "        #                         1. Get new batch                            #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        latent_real, y_real = next(iter(train_loader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        latent_real, y_real, y_fake = latent_real.to(DEVICE), y_real.to(DEVICE), y_fake.to(DEVICE)\n",
    "        \n",
    "        bs = latent_real.size(0)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        #                        2. Train Discriminator                       #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        dis_real = D(latent_real, y_real)\n",
    "\n",
    "        latent_real, y_real = next(iter(train_loader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        latent_real, y_real, y_fake = latent_real.to(DEVICE), y_real.to(DEVICE), y_fake.to(DEVICE)\n",
    "        \n",
    "        latent_fake = G(latent_real, y_fake)\n",
    "        dis_fake = D(latent_fake, y_fake)\n",
    "        \n",
    "        dis_fakes.append(dis_fake)\n",
    "\n",
    "        d_loss = criterion_d(dis_real, dis_fake)\n",
    "        # d_loss = BCE(dis_real, torch.ones_like(dis_real).to(DEVICE)) + \\\n",
    "        #          BCE(dis_fake, torch.zeros_like(dis_fake).to(DEVICE))\n",
    "        \n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        d_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         3. Train Generator                          #\n",
    "        # =================================================================== #\n",
    "        latent_real, y_real = next(iter(train_loader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        latent_real, y_real, y_fake = latent_real.to(DEVICE), y_real.to(DEVICE), y_fake.to(DEVICE)\n",
    "        \n",
    "        latent_fake = G(latent_real, y_fake)\n",
    "        dis_fake = D(latent_fake, y_fake)\n",
    "        \n",
    "        latent_cyclic =  G(latent_fake, y_real)\n",
    "        cyclic_loss = F.l1_loss(latent_cyclic, latent_real)\n",
    "        \n",
    "        dis_fakes.append(dis_fake)\n",
    "        dis_reals.append(dis_real)\n",
    "        \n",
    "        # latent_id =  G(latent_real, y_real)\n",
    "        # identity_loss = F.l1_loss(latent_id, latent_real)\n",
    "        \n",
    "        g_loss = criterion_g(dis_fake) + cyclic_loss\n",
    "        # g_loss = BCE(dis_fake, torch.ones_like(dis_fake).to(DEVICE)) + cyclic_loss\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            print(\"Step: {} | D_loss: {:.3f} | G_loss: {:.3f} | Cyclic loss: {:.3f}\".format(iteration, \n",
    "                                                                        d_loss.item(), \n",
    "                                                                        g_loss.item(),\n",
    "                                                                        cyclic_loss.item()))\n",
    "            \n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_fake = G(latent_real, y_fake)\n",
    "                x_decoded = ae.decoder(latent_fake.view(bs, -1, 2, 2)).detach().cpu().permute(0, 2, 3, 1)\n",
    "                x_decoded = (x_decoded - x_decoded.min()) / (x_decoded.max() - x_decoded.min())\n",
    "            \n",
    "            images = ae.decoder(latent_real.view(bs, -1, 2, 2)).detach().cpu().permute(0, 2, 3, 1)\n",
    "            images = (images - images.min()) / (images.max() - images.min())\n",
    "            target = np.array([train_dataset.one2multi[a.item()] for a in y_fake]).astype(int)\n",
    "            target[target == -1] = 0\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=2, ncols=8, figsize=(20, 5))\n",
    "            # print(y_fake[:8])\n",
    "            for i in range(2):\n",
    "                for j in range(8):\n",
    "                    if i == 0:\n",
    "                        name = '\\n'.join([name + f\": {val}\" for name, val in zip(features, target[j])])\n",
    "                        ax[i][j].set_title(name, fontsize=14)\n",
    "                        ax[i][j].imshow(images[j], aspect='auto')\n",
    "                        ax[i][j].axis('off')\n",
    "                    else:\n",
    "                        ax[i][j].imshow(x_decoded[j], aspect='auto')\n",
    "                        ax[i][j].axis('off')\n",
    "            plt.tight_layout(pad=0)\n",
    "            plt.show()\n",
    "            G.train()\n",
    "        \n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(torch.cat([a.detach().cpu().squeeze() for a in dis_reals], dim=0), \n",
    "         bins=1000, histtype='step', color='darkviolet', label='Real');\n",
    "plt.xticks(fontsize=14)\n",
    "plt.hist(torch.cat([a.detach().cpu().squeeze() for a in dis_fakes], dim=0), \n",
    "         bins=1000, histtype='step', color='g', label='Fake');\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(torch.cat([a.detach().cpu().squeeze() for a in dis_reals], dim=0), \n",
    "         bins=1000, histtype='step', color='darkviolet', label='Real');\n",
    "plt.xticks(fontsize=14)\n",
    "plt.hist(torch.cat([a.detach().cpu().squeeze() for a in dis_fakes], dim=0), \n",
    "         bins=1000, histtype='step', color='g', label='Fake');\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "num_classes = 2\n",
    "G = ConcatGenerator(hidden_size=128, num_classes=num_classes).to(DEVICE)\n",
    "D = ProjectionDiscriminator(hidden_size=128, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "args =  {'dataset': 'MNIST',\n",
    "         'eval_each': 10,\n",
    "         'epochs': 101,\n",
    "         'log_dir': 'CelebA64_256_v2/',\n",
    "         'device': DEVICE,\n",
    "         'weight_decay': 1e-05,\n",
    "         'depth': 16,\n",
    "         'gamma': 0.2,\n",
    "         'lmbda': 0.5,\n",
    "         'batch_norm': False,\n",
    "         'batch_size': 64,\n",
    "         'colors': 3,\n",
    "         'latent_width': 4, # Bottleneck HW\n",
    "         'width': 128, # Means 4 downsampling blocks\n",
    "         'latent': 32, # Bottleneck channels\n",
    "         'n_classes': 10,\n",
    "         'advdepth': 16,\n",
    "         'lr': 0.0001}\n",
    "\n",
    "scales = int(round(math.log(args['width'] // args['latent_width'], 2)))\n",
    "ae = Autoencoder(scales=scales,depth=args['depth'],latent=args['latent'],colors=args['colors']).to(args['device']).eval()\n",
    "\n",
    "ae.load_state_dict(torch.load('acai_64.pt', map_location=args['device']))\n",
    "\n",
    "optimizer_g = torch.optim.Adam(G.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "optimizer_d = torch.optim.Adam(D.parameters(), lr=1e-4, betas=(0, 0.9))\n",
    "\n",
    "loss_type = 'hinge'\n",
    "\n",
    "if loss_type != 'bce':\n",
    "    criterion_d = DisLoss(loss_type)\n",
    "    criterion_g = GenLoss(loss_type)\n",
    "\n",
    "G.train();\n",
    "D.train();\n",
    "\n",
    "BCE = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "dis_fakes = []\n",
    "dis_reals = []\n",
    "for epoch in range(10):\n",
    "    for _ in tqdm_notebook(range(len(train_loader))):\n",
    "        # =================================================================== #\n",
    "        #                         1. Get new batch                            #\n",
    "        # =================================================================== #\n",
    "        \n",
    "        latent_real, y_real = next(iter(train_loader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        latent_real, y_real, y_fake = latent_real.to(DEVICE), y_real.to(DEVICE), y_fake.to(DEVICE)\n",
    "        cond_true = nn.functional.one_hot(y_real, num_classes)\n",
    "        cond_fake = nn.functional.one_hot(y_fake, num_classes)\n",
    "        \n",
    "        bs = latent_real.size(0)\n",
    "        \n",
    "        # =================================================================== #\n",
    "        #                        2. Train Discriminator                       #\n",
    "        # =================================================================== #\n",
    "        for _ in range(5):\n",
    "            dis_real = D(latent_real, y_real)\n",
    "\n",
    "            latent_real, y_real = next(iter(train_loader))\n",
    "            y_fake = sample_target_labels(y_real, num_classes)\n",
    "            latent_real, y_real, y_fake = latent_real.to(DEVICE), y_real.to(DEVICE), y_fake.to(DEVICE)\n",
    "            cond_true = nn.functional.one_hot(y_real, num_classes)\n",
    "            cond_fake = nn.functional.one_hot(y_fake, num_classes)\n",
    "\n",
    "            latent_fake = G(latent_real, cond_fake)\n",
    "            dis_fake = D(latent_fake, y_fake)\n",
    "\n",
    "            dis_fakes.append(dis_fake)\n",
    "\n",
    "            if loss_type == 'bce':\n",
    "                d_loss = BCE(dis_real, torch.ones_like(dis_real).to(DEVICE)) + \\\n",
    "                         BCE(dis_fake, torch.zeros_like(dis_fake).to(DEVICE))\n",
    "            else:\n",
    "                d_loss = criterion_d(dis_real, dis_fake)\n",
    "\n",
    "            optimizer_d.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        # =================================================================== #\n",
    "        #                         3. Train Generator                          #\n",
    "        # =================================================================== #\n",
    "        latent_real, y_real = next(iter(train_loader))\n",
    "        y_fake = sample_target_labels(y_real, num_classes)\n",
    "        latent_real, y_real, y_fake = latent_real.to(DEVICE), y_real.to(DEVICE), y_fake.to(DEVICE)\n",
    "        cond_true = nn.functional.one_hot(y_real, num_classes)\n",
    "        cond_fake = nn.functional.one_hot(y_fake, num_classes)\n",
    "        \n",
    "        latent_fake = G(latent_real, cond_fake)\n",
    "        dis_fake = D(latent_fake, y_fake)\n",
    "        \n",
    "        latent_cyclic =  G(latent_fake, cond_true)\n",
    "        cyclic_loss = F.l1_loss(latent_cyclic, latent_real)\n",
    "        \n",
    "        dis_fakes.append(dis_fake)\n",
    "        dis_reals.append(dis_real)\n",
    "        \n",
    "        # latent_id =  G(latent_real, y_real)\n",
    "        # identity_loss = F.l1_loss(latent_id, latent_real)\n",
    "        \n",
    "        if loss_type == 'bce':\n",
    "            g_loss = BCE(dis_fake, torch.ones_like(dis_fake).to(DEVICE)) + cyclic_loss\n",
    "        else:\n",
    "            g_loss = criterion_g(dis_fake) + cyclic_loss\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        g_loss.backward()\n",
    "        optimizer_g.step()\n",
    "        \n",
    "        if iteration % 50 == 0:\n",
    "            print(\"Step: {} | D_loss: {:.3f} | G_loss: {:.3f} | Cyclic loss: {:.3f}\".format(iteration, \n",
    "                                                                        d_loss.item(), \n",
    "                                                                        g_loss.item(),\n",
    "                                                                        cyclic_loss.item()))\n",
    "            \n",
    "            \n",
    "        if iteration % 100 == 0:\n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                latent_fake = G(latent_real, cond_fake)\n",
    "                x_decoded = ae.decoder(latent_fake.view(bs, -1, 2, 2)).detach().cpu().permute(0, 2, 3, 1)\n",
    "                x_decoded = (x_decoded - x_decoded.min()) / (x_decoded.max() - x_decoded.min())\n",
    "            \n",
    "            images = ae.decoder(latent_real.view(bs, -1, 2, 2)).detach().cpu().permute(0, 2, 3, 1)\n",
    "            images = (images - images.min()) / (images.max() - images.min())\n",
    "            target = np.array([train_dataset.one2multi[a.item()] for a in y_fake]).astype(int)\n",
    "            target[target == -1] = 0\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=2, ncols=8, figsize=(20, 5))\n",
    "            # print(y_fake[:8])\n",
    "            for i in range(2):\n",
    "                for j in range(8):\n",
    "                    if i == 0:\n",
    "                        name = '\\n'.join([name + f\": {val}\" for name, val in zip(features, target[j])])\n",
    "                        ax[i][j].set_title(str(name), fontsize=14)\n",
    "                        ax[i][j].imshow(images[j], aspect='auto')\n",
    "                        ax[i][j].axis('off')\n",
    "                    else:\n",
    "                        ax[i][j].imshow(x_decoded[j], aspect='auto')\n",
    "                        ax[i][j].axis('off')\n",
    "            plt.tight_layout(pad=0)\n",
    "            plt.show()\n",
    "            G.train()\n",
    "        \n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(torch.cat([a.detach().cpu().squeeze() for a in dis_reals], dim=0), \n",
    "         bins=1000, histtype='step', color='darkviolet', label='Real');\n",
    "plt.xticks(fontsize=14)\n",
    "plt.hist(torch.cat([a.detach().cpu().squeeze() for a in dis_fakes], dim=0), \n",
    "         bins=1000, histtype='step', color='g', label='Fake');\n",
    "plt.legend(fontsize=14)\n",
    "plt.xticks(fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
