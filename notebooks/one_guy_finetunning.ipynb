{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T08:04:38.623600Z",
     "start_time": "2020-05-27T08:04:38.602600Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.nn.functional import softplus\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from math import log2, ceil\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from model import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "\n",
    "DEVICE = 'cuda:2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SingleFaceDataset, BatchCollate\n",
    "from albumentations import HorizontalFlip, Compose, KeypointParams\n",
    "from albumentations.pytorch import ToTensor\n",
    "\n",
    "transform = Compose([HorizontalFlip(), ToTensor()], keypoint_params=KeypointParams(format='xy'))\n",
    "transform_ci = Compose([ToTensor()], keypoint_params=KeypointParams(format='xy'))\n",
    "dataset = SingleFaceDataset(\n",
    "    root='/root/img_dataset/id00061/',\n",
    "    center_identity_size=4,\n",
    "    center_identity_step=32,\n",
    "    transform=transform,\n",
    "    transform_ci=transform_ci,\n",
    "    size=3200\n",
    ")\n",
    "\n",
    "batch_collate = BatchCollate()\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=batch_collate,\n",
    "    drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load `pretrained model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-27T08:04:38.623600Z",
     "start_time": "2020-05-27T08:04:38.602600Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_filename):\n",
    "    d = torch.load(checkpoint_filename, map_location=lambda storage, loc: storage)\n",
    "    \n",
    "    decoder = model.decoder\n",
    "    encoder = model.encoder\n",
    "    mapping_tl = model.mapping_tl\n",
    "    mapping_fl = model.mapping_fl\n",
    "    dlatent_avg = model.dlatent_avg\n",
    "    \n",
    "    model_dict = {\n",
    "        'discriminator_s': encoder,\n",
    "        'generator_s': decoder,\n",
    "        'mapping_tl_s': mapping_tl,\n",
    "        'mapping_fl_s': mapping_fl,\n",
    "        'dlatent_avg': dlatent_avg\n",
    "    }\n",
    "    \n",
    "    for key in model_dict.keys():\n",
    "        model_dict[key].load_state_dict(d['models'][key])\n",
    "        \n",
    "    \n",
    "def save_reconstructions(name, original, reconstruction, nrows=6):\n",
    "    \"\"\"\n",
    "    original, reconstruction - type: list, e.g. original = [x, x_hat], reconstruction = [G(E(x)), G(E(x_hat))]\n",
    "    \n",
    "    [[orig_x, rec_x], [orig_x, rec_x], [orig_x, rec_x]]\n",
    "    [[orig_x_hat, rec_x_hat], [orig_x_hat, rec_x_hat], [orig_x_hat, rec_x_hat]]\n",
    "    \n",
    "    \"\"\"\n",
    "    tensor = []\n",
    "    for orig, rec in zip(original, reconstruction):        \n",
    "        tensor.append(torch.cat([torch.cat([orig.split(1)[i], rec.split(1)[i]], dim=0) for i in range(nrows//2)], dim=0))\n",
    "    \n",
    "    save_image(torch.cat(tensor, dim=0), name, nrow=nrows, padding=1, normalize=True, range=(-1, 1))\n",
    "    \n",
    "alae = Model(latent_size=256, layer_count=6, maxf=256, startf=64, mapping_layers=8, dlatent_avg_beta=0.995, style_mixing_prob=0,\n",
    "             generator = 'GeneratorDefault',\n",
    "             encoder = 'EncoderDefault')\n",
    "        \n",
    "load_checkpoint(alae, 'training_artifacts/celeba/model_final.pth')\n",
    "alae = alae.to(DEVICE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG `loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh2sigmoid(batch):\n",
    "    return batch.div(2).add(0.5)\n",
    "\n",
    "\n",
    "class VGGExtractor(nn.Module):\n",
    "    def __init__(self, vgg):\n",
    "        super(VGGExtractor, self).__init__()\n",
    "        \n",
    "        mean = torch.FloatTensor([0.485, 0.456, 0.406])[None, :, None, None]\n",
    "        self.register_buffer('mean', mean)\n",
    "\n",
    "        std = torch.FloatTensor([0.229, 0.224, 0.225])[None, :, None, None]\n",
    "        self.register_buffer('std', std)\n",
    "        \n",
    "        self.relu0_1 = vgg[0:2]\n",
    "        self.relu1_1 = vgg[2:7]\n",
    "        self.relu2_1 = vgg[7:12]\n",
    "        self.relu3_1 = vgg[12:21]\n",
    "        self.relu4_1 = vgg[21:30]\n",
    "        \n",
    "    def forward(self, x, level=1):\n",
    "        x = (x - self.mean)/self.std\n",
    "        \n",
    "        extracted_featrues = []\n",
    "        for block in [self.relu0_1, self.relu1_1, self.relu2_1, self.relu3_1, self.relu4_1][:level]:\n",
    "            x = block(x)\n",
    "            extracted_featrues.append(x)\n",
    "            \n",
    "        return extracted_featrues\n",
    "    \n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        self.feature_exctracor = VGGExtractor(models.vgg19(pretrained=True).features).to(DEVICE)\n",
    "        self.feature_exctracor.eval();\n",
    "\n",
    "    def forward(self, inp1, inp2, level):\n",
    "        coefs = [1, 1/4, 1/8]\n",
    "        features1, features2 = self.feature_exctracor(inp1, level), self.feature_exctracor(inp2, level)\n",
    "        \n",
    "        loss = 0\n",
    "        for i in range(level):\n",
    "            loss += coefs[i] * (features1[i] - features2[i]).pow(2).mean()\n",
    "        return loss\n",
    "    \n",
    "vgg_loss = VGGLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.mkdir('Finetune_id00061_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencode(model, batch):\n",
    "    codes = model.encode(batch.to(DEVICE), 5, 1)[0].repeat(1, 12, 1)\n",
    "    return model.decoder(codes, lod=5, blend=1, noise='batch_constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False\n",
    "\n",
    "alae.train()\n",
    "\n",
    "experiment_name = 'Finetune_id00061_correct'\n",
    "\n",
    "try:\n",
    "    os.mkdir(experiment_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "SAVE_IMAGES_EACH = 98\n",
    "EPOCHS = 1000\n",
    "reconstruction_loss = []\n",
    "\n",
    "BS = 32\n",
    "scale = 128\n",
    "\n",
    "optimizer_G = Adam(alae.decoder.parameters(), betas=(0, 0.99), lr=1e-4)\n",
    "best_loss = 100\n",
    "\n",
    "for epoch in range(EPOCHS):    \n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Epoch: [{epoch+1}/{EPOCHS}]'):\n",
    "                                 \n",
    "        real_samples = batch['image'].to(DEVICE).add(-0.5).mul(2)         \n",
    "        x_reconstruction = autoencode(alae, real_samples).clamp(-1.2, 1.2)\n",
    "        \n",
    "        loss = vgg_loss(tanh2sigmoid(real_samples), tanh2sigmoid(x_reconstruction), 3)\n",
    "        reconstruction_loss.append(loss.item())\n",
    "                                 \n",
    "        optimizer_G.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        if ((batch_idx+1) % SAVE_IMAGES_EACH) == 0:                                \n",
    "\n",
    "            name = f'Epoch{epoch}_Img{batch_idx}.png'\n",
    "            save_reconstructions(os.path.join(experiment_name, name),\n",
    "                                 [real_samples.cpu().detach(), real_samples.cpu().detach()[-16:]],\n",
    "                                 [x_reconstruction.cpu().detach(), x_reconstruction.cpu().detach()[-16:]], nrows=6)\n",
    "                                 \n",
    "            Image.open(os.path.join(experiment_name, name)).resize((1536, 512)).save(os.path.join(experiment_name, name))\n",
    "\n",
    "            # Save losses\n",
    "            pd.DataFrame(reconstruction_loss).to_csv(os.path.join(experiment_name, 'stats.csv'), index=False)\n",
    "        \n",
    "            if min(reconstruction_loss[-100:]) < best_loss:\n",
    "                torch.save({'AE': alae.state_dict(),\n",
    "                            'optAE': optimizer_G.state_dict()},\n",
    "                            os.path.join(experiment_name, f'BestModel.pt'))\n",
    "        \n",
    "    if epoch % 20 == 0:\n",
    "        torch.save({'AE': alae.state_dict(),\n",
    "                    'optAE': optimizer_G.state_dict()},\n",
    "                    os.path.join(experiment_name, f'Finetuned_Epoch{epoch}.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
